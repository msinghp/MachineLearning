---
title: ''
output:
  html_document:
    keep_md: yes
---
#### Assessment of the eficacy of Exercise Activity

####Loading the required Libraries

```{r echo=FALSE}
library(randomForest)
library(class) 
library(e1071)      		   
library(caret)
```




#### Executive Summary

The exercise activity data provided  was analyzed to find out whether it was possible to predict how well the exercise was performed. Using the randomforest algorithm yielded  a 99% accuracy in predicting how well the exercises wa sperformed

####Introduction

Devices such as Jawbone Up, Nike FuelBand, and Fitbit  have been utilized  by tech geek health enthusiasts  to collect exercise data in an effort to improve their health. While the amount of exercise is often collected, how well the exercise is done is seldom quantified. The current  data tracks data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants who were asked to perform barbell lifts correctly and incorrectly in 5 different ways.


####Data Description



#### Feature Selection

Features were selected using the following process

1. Removing the first eight columns since they contain pesonal identification data.

2. Remove all calculated fields such as standard deviation since they are derived from the existing data

3. Remove all columns that mostly contain NAs

The above process resulted in the selection of 36 features

```{r, echo=TRUE}
tr <- read.csv("pml-training.csv", na.strings=c("#DIV/0!","NA") )
tr <- tr[,c(9:160)]
r <- grep('kur*|std*|skew*|max*|min*|var*|avg*|tot*|user|new|X|num',colnames(tr))
tr <- tr[-r]
nacols <- c((colSums(!is.na(tr[,-ncol(tr)])) >= 0.6*nrow(tr)))
tr <- tr[,nacols]
tr$classe <- factor(tr$classe)

```

#####How model was built

The model was generated as follows:

The pml-training data was split 60:40 into a training set and a testing set. The training file was used to trian the model using the random forest algorithm. The tesing file was used to test the model generated by the random forest algorithm. The random forest algorithm was chosen for the following reasons ( modified from https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm#overview)

1. Compared to other algorithms,it has high accuracy
2. Runs efficiently on large data sets
3. It can handle large numbers of of input variables

##### Running the model against the tesing file

The folloing results were obtained when the  trained model was run against the testing file

```{r, echo=TRUE}

trainIndex <- createDataPartition(tr$classe, p=0.60, list=FALSE)
training<- tr[ trainIndex,]
testing<- tr[-trainIndex,]
model <- randomForest(classe~.,data=training)
testclass <- predict(model, testing)
cfMatrix <- confusionMatrix(testclass, testing$classe)
cfMatrix 


```
 


Cross validation

Expected out of sample error





##### Test Cases

The ml-testing.csv file contained 20 test cases. The file was subjected to the same data cleaning techniques as was used with the pml-training.csv file that was used to generate the model

```{r, echo=TRUE}

ev <- read.csv("pml-testing.csv", na.strings=c("#DIV/0!") )
ev <- ev[,c(9:160)]
ev <- ev[-r]
ev <- ev[,nacols]
testclass1 <- predict(model, ev)
testclass1



```





